[["index.html", "Text-Analysis-in-Python Chapter 1 Basic Python Knowledge 1.1 Shortcuts in Jupyter Notebook 1.2 Make one Python script run another 1.3 Concepts 1.4 Open a file 1.5 Set a path 1.6 Dataframe 1.7 Other symbols 1.8 Loops 1.9 Lambda Function 1.10 General Funtions", " Text-Analysis-in-Python Zoe Li 2022-04-28 Chapter 1 Basic Python Knowledge ##change work directory import os os.getcwd() os.chdir(“/Users…”) Check the version of Python from platform import python_version print(python_version()) 1.1 Shortcuts in Jupyter Notebook Source: Note that the shortcuts are for Windows and Linux users. Anyway, for the Mac users, they’re different buttons for Ctrl, Shift, and Alt: • Ctrl: command key ⌘ • Shift: Shift ⇧ • Alt: option ⌥ First, we need to know that they are 2 modes in the Jupyter Notebook App: command mode and edit mode. I’ll start with the shortcuts shared between the two modes. Shortcuts in both modes: • Shift + Enter run the current cell, select below • Ctrl + Enter run selected cells • Alt + Enter run the current cell, insert below • Ctrl + S save and checkpoint While in command mode (press Esc to activate): • Enter take you into edit mode • H show all shortcuts • Up select cell above • Down select cell below • Shift + Up extend selected cells above • Shift + Down extend selected cells below • A insert cell above • B insert cell below • X cut selected cells • C copy selected cells • V paste cells below • Shift + V paste cells above • D, D (press the key twice) delete selected cells • Z undo cell deletion • S Save and Checkpoint • Y change the cell type to Code • M change the cell type to Markdown • P open the command palette. This dialog helps you run any command by name. It’s really useful if you don’t know some shortcut or when you don’t have a shortcut for the wanted command. • Shift + Space scroll notebook up • Space scroll notebook down While in edit mode (pressEnter to activate) • Esc take you into command mode • Tab code completion or indent • Shift + Tab tooltip • Ctrl + ] indent • Ctrl + [ dedent • Ctrl + A select all • Ctrl + Z undo • Ctrl + Shift + Z or Ctrl + Y redo • Ctrl + Home go to cell start • Ctrl + End go to cell end • Ctrl + Left go one word left • Ctrl + Right go one word right • Ctrl + Shift + P open the command palette • Down move cursor down • Up move cursor up 1.2 Make one Python script run another Source: https://stackoverflow.com/questions/7974849/how-can-i-make-one-python-file-run-another There are more than a few ways. I’ll list them in order of inverted preference (i.e., best first, worst last): 1. Treat it like a module: import file. This is good because it’s secure, fast, and maintainable. Code gets reused as it’s supposed to be done. Most Python libraries run using multiple methods stretched over lots of files. Highly recommended. Note that if your file is called file.py, your import should not include the .py extension at the end. 2. The infamous (and unsafe) exec command: Insecure, hacky, usually the wrong answer. Avoid where possible. • execfile(‘file.py’) in Python 2 • exec(open(‘file.py’).read()) in Python 3 3. Spawn a shell process: os.system(‘python file.py’). Use when desperate. 4. Put this in main.py: 5. #!/usr/bin/python import yoursubfile 6. Put this in yoursubfile.py 7. #!/usr/bin/python print(“hello”) 8. Run it: python main.py 9. It prints: hello 1.3 Concepts () - tuple A tuple is a sequence of items that can’t be changed (immutable). [] - list A list is a sequence of items that can be changed (mutable). convert a list to a string list1 = [‘1’, ‘2’, ‘3’] str1 = ’’.join(list1) Or if the list is of integers, convert the elements before joining them. list1 = [1, 2, 3] str1 = ’’.join(str(e) for e in list1) check if a list contain some string of another list my_list = [‘abc-123’, ‘def-456’, ‘ghi-789’, ‘abc-456’, ‘def-111’, ‘qwe-111’] bad = [‘abc’, ‘def’] [e for e in bad if e in ‘’.join(my_list)] #return item in bad that is contained in my_list [i for e in bad for i in my_list if e in i] #returns items in my_list that contains items in bad Convert a list to a string list_of_strings = [‘one’, ‘two’, ‘three’] my_string = ’’ for word in list_of_strings: my_string += str(word + “,”) my_string = my_string[:-1] my_string += ‘.’ print(“Final result:”, my_string) Parallel adding items of lists within a list def add_lists(outside_lst): result_lst = list() for j in range(0, len(outside_lst[0])): tmp = 0 for i in range(0, len(outside_lst)): tmp = tmp + outside_lst[i][j] result_lst.append(tmp) return result_lst {} - dictionary or set A dictionary is a list of key-value pairs, with unique keys (mutable). From Python 2.7/3.1, {} can also represent a set of unique values (mutable). dist.items() dist.values() dist.keys() change values of a dictionary, x is a list for city, team in dic1.items(): for x in list2: if team in x: dic1[city] = x String Use variables in string regex TEXTO = “Var” subject = r”Var” if re.search(rf”?=){TEXTO}\\boundary(?!)“, subject, re.IGNORECASE): print(”match”) ## Packages/Modules 1.3.1 install packages pip install language_tool_python Install a specific version pip install spacy==2.1.3 pip install “prettytable&lt;2.0” List all packages intalled by pip pip list #include editable ones pip freeze Save the package list to a text file and uninstall all of them pip freeze &gt; requirements.txt pip uninstall -r requirements.txt !python -m spacy download en Check the version of pip pip –version Run Anaconda as an administrator conda install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz –no-deps python -m spacy download en_core_web_sm python -m spacy download en 1.3.2 uninstall packages pip uninstall spacy 1.3.3 upgrade packages: the older version is automatically replaced when you install a newer version import os import pandas as pd Pandas represents NaN as a floating point number; similar to none, but not equivalent. import numpy as np Regular expression import re from nltk import sent_tokenize from nltk import word_tokenize from nltk import pos_tag from nltk import RegexpParser import spacy from spacy import displacy from nltk import Tree Networks import networkx as nx 1.3.4 Common Errors Could not install packages due to an OSError: [WinError 5] Access is denied: Solution: add –user pip install spacy==2.3.5 –user 1.4 Open a file df = pd.read_csv(‘datasets/Admission_Predict.csv’) df = pd.read_excel(‘datasets/Admission_Predict.xls’) #or xlsx cities = pd.read_html(“wikipedia_data.html”) We notice that by default index starts with 0 while the students’ serial number starts from 1. If you jump back to the CSV output you’ll deduce that pandas has create a new index. Instead, we can set the serial NO. as the index if we want to by using the index_col. df = pd.read_csv(‘datasets/Admission_Predict.csv’, index_col=0) read a text file as a table import pandas as pd df = pd.read_csv(‘file.txt’, sep=‘’) df.columns = [‘Name’,‘Hours’,‘Total Pay’] or df = pd.read_csv(“fixtures.txt”, index_col=False, names=columns) 1.5 Set a path path = “C:\\Users…” dfsource = pd.read_csv(path + “abcd.csv”) 1.6 Dataframe Build a dataframe d1 = {‘a’: 4, ‘b’: 7, ‘c’:-5, ‘d’:3} d2 = pd.Series(d1) build a matrix b = np.array([[1,2,3],[4,5,6]]) b 1 2 3 4 5 6 here, the dimension means the number of rows, shape means dimensions in R b.ndim #2 b.shape #(2, 3) Let’s look at the first or last few rows. df.head() df.tail() see column names df.columns for col in newdata.columns: print(col) see one column df[“gre score”].head() see two columns df[[“gre score”,“toefl score”]].head() see the table conditional on a column df[df[“gre score”]&gt;320].head() Or just see the rows meet some conditions. df[df[‘chance of admit’] &gt; 0.7].head() see the table conditional on a column (two conditions) &amp; ( ) (df[‘chance of admit’] &gt; 0.7) &amp; (df[‘chance of admit’] &lt; 0.9) or gt=greater than; lt=lower than df[‘chance of admit’].gt(0.7) &amp; df[‘chance of admit’].lt(0.9) df[‘chance of admit’].gt(0.7).lt(0.9) check missing values mask=df.isnull() mask.head(10) #missing values gave “true” drop rows with missing values df.dropna().head(10) fill missing values as 0 df.fillna(0, inplace=True) sort df = df.set_index(‘time’) #sort by time df = df.sort_index() df = df.reset_index() df = df.set_index([‘time’, ‘user’]) #sort by time then by user sort by a column df.sort_values(“Population”, asending=False, inplace=True) Find unique values df[‘SUMLEV’].unique() max np.max(df[“HAD_CPOX”]) df[‘CitationRatio’].max() df.groupby([‘Continent’])[‘% Renewable’].max() #group by find the row label which has the max % Renewable answer_one()[‘% Renewable’].idxmax() df.iloc[df[‘columnX’].argmax()] df.loc[df[‘CitationRatio’] == df[‘CitationRatio’].max()] #get the entire row df[df[‘CitationRatio’] == df[‘CitationRatio’].max()].index.tolist() return df[‘Population’].nlargest(3).index.tolist()[2] #return the top 3, strings, get the 3rd one rowmeans df.mean(axis=1) df[‘mean’] = df.mean(axis=1) #add a column of row means columnmeans answer_one()[‘Energy Supply per Capita’].mean(axis=0) Fill with group means df[‘value’] = df.groupby(‘indicator’)[‘value’].transform(‘mean’) loc &amp; iloc loc is label-based, which means that you have to specify rows and columns based on their row and column labels. iloc is integer index based, so you have to specify rows and columns by their integer index like you did in the previous exercise. They are attributes, not methods. so use [ ], not ( ). In [ ], the former part is the rows selected, the latter part is the columns selected. Here, from the first row to the second last row, the columns 0,3,5,6,7,8 are selected. Although the data frame has column names… cities=cities.iloc[:-1,[0,3,5,6,7,8]] data.iloc[:,0] data.iloc[:,1] # second column of data frame (last_name) data.iloc[:,-1] # last column of data frame (id) locate a value by its row label and column label newdf[‘2015’].loc[‘United Kingdom’] Example: 1.School1 is one of the row names df.loc[‘school1’] #will return the row of school1 and all of the columns 2.Name is one of the column names df.loc[‘school1’, ‘Name’] #will return the row of school1 and the column of name 3.Both Michigan and Washtenaw are row names df.loc[‘Michigan’, ‘Washtenaw County’] df.loc[ [(‘Michigan’, ‘Washtenaw County’), (‘Michigan’, ‘Wayne County’)]] Replace values df.replace([1, 3], [100, 300]) newdata.replace([“Republic of Korea”, “United States of America”, “United Kingdom of Great Britain and Northern Ireland”, “China, Hong Kong Special Administrative Region”], [“South Korea”, “United States”, “United Kingdom”, “Hong Kong”]) df.replace(to_replace=“.*.html$”, value=“webpage”, regex=True) newdata[newdata==“…”]=np.NaN #replace missing data … with NaN values Replace values by row and column indecis newdf.iloc[188:376, 0:] = 99999 Replace a value by row index and column name df.at[189, column] = “something” multiply a column newdata[‘Energy Supply’] = newdata[‘Energy Supply’]*1000000 Convert the Population Estimate series to a string with thousands separator (using commas). Use all significant digits (do not round the results). df[“Population”] = df[“Population”].map(‘{:,}’.format) df[‘new_column_name’] = df[‘column_name’].map(‘{:,.2f}’.format) #two digits after the decimal point Rename df.rename(columns={‘LOR’: ‘Letter of Recommendation’}) newdata.columns = [‘Country’, ‘Energy Supply’, ‘Energy Supply per Capita’, ‘% Renewable’] #no matter what the old column names are, they would be replaced newdata[‘Country’] = newdata[‘Country’].str.replace(r’‘,’’) #replace the non-letters of values in the column “country” newdata[‘Country’] = newdata[‘Country’].str.replace(r’\\(.*\\)‘,’’) #replace the part in ( ) new_df.rename(mapper=str.strip, axis=‘columns’) #delete white spaces of column or row names change all of the column names to lower case cols = list(df.columns) Then a little list comprehenshion cols = [x.lower().strip() for x in cols] Then we just overwrite what is already in the .columns attribute df.columns=cols set the first row as column names new_header = GDP.iloc[0] #grab the first row for the header GDP = GDP[1:] #take the data less the header row GDP.columns = new_header Drop a column First, let’s make a copy of a DataFrame using .copy() copy_df = df.copy() Now lets drop the “Name” column in this copy. copy_df.drop(“Name”, inplace=True, axis=1) There is a second way to drop a column, and that’s directly through the use of the indexing operator, using the del keyword. This way of dropping data, however, takes immediate effect on the DataFrame and does not return a view. del copy_df[‘Class’] drop colums 0 and 1(attention: written as 0:2) data.drop(data.columns[0:2], axis=1, inplace=True) drop rows #cannot written as 0:17 data.drop(data.index[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]], inplace=True) reset index after dropping some rows data.reset_index(drop=True, inplace=True) Add a column df[‘ClassRanking’] = None Add a new column based on the other one (whether it’s greater than the median or not) df[‘HighRenew’] = (df[‘% Renewable’] &gt;= df[‘% Renewable’].median()).astype(int) df[‘HighRenew’] = df[‘% Renewable’].apply(lambda x: 1 if (x &gt;= df[‘%Renewable’].median()) else 0) #both works Add a column based on the other column and a dictionary df[‘Continent’] = df.index.map(ContinentDict) #here ContinentDict is a dictionary, df.index can be also a column of df. Add n columns for n in range(26): dfsource[n] = ‘nan’ Boolean mask We can use this to hide the rows with false value with NA, then delete rows with NA values df.where(admit_mask).head() df.where(admit_mask).dropna().head() admit_mask=df[‘chance of admit’] &gt; 0.7 Select rows and columns • df.iloc[[4]] #select row number 4 • df.iloc[[2, 3, 4]] #select rows 2, 3, and 4 select = newdata.loc[newdata[‘Country’] == ‘Algeria’] ##a row whose country is Algeria print(select) get a subset conditional on a column df=df[df[‘SUMLEV’] == 50] or use filter df.groupby([‘ID1’, ‘ID2’, ‘ID3’]).filter(lambda x: len(x[‘any_variable’]) &lt;= 5) get a subset conditional on two columns data_tagged[(data_tagged.V1 == 1)|(data_tagged.V2 == 1)] check the size of a subset data_tagged_positve[“V1”] == “some_value”).sum() get a subset by column names newdf = df[[‘2006’,‘2007’, ‘2008’, ‘2009’, ‘2010’, ‘2011’, ‘2012’, ‘2013’, ‘2014’, ‘2015’]] or columns_to_keep = [‘STNAME’,‘CTYNAME’,‘BIRTHS2010’,‘BIRTHS2011’,‘BIRTHS2012’,‘BIRTHS2013’, ‘BIRTHS2014’,‘BIRTHS2015’,‘POPESTIMATE2010’,‘POPESTIMATE2011’, ‘POPESTIMATE2012’,‘POPESTIMATE2013’,‘POPESTIMATE2014’,‘POPESTIMATE2015’] df = df[columns_to_keep] Remove duplicates df.drop_duplicates Split the values of one column into two pattern=“(?P1)(?:. )(?P[]*$)” Now call extract names=df[“President”].str.extract(pattern).head() df[“First”]=names[“First”] df[“Last”]=names[“Last”] Combine two dataframes (one on top of the other) newdf = df.append(df1, ignore_index=True) Subset based on another dataframe training[~training.CASE_ID.isin(df1.CASE_ID)] Plots Histagram import matplotlib.pyplot as plt import seaborn as sns plt.hist(data_tagged_positvenew.groupby([‘ID1’, ‘ID2’, ‘ID3’]).size(), color = ‘blue’, edgecolor = ‘black’, bins = int(180/5)) 1.7 Other symbols print(‘Original Article: %s’ % (article)) for token in doc: if token.text != token.lemma_: print(‘Original : %s, New: %s’ % (token.text, token.lemma_)) 1.8 Loops 1.8.1 Symbols &amp; Expressions • When you say: • x |= y ; • …it’s the same as saying: • x = x | y ; • …just as: • x +=y ; • …is the same as: • x = x + y ; no remainder if number % 2 == 0 #example, number is factorial of 2 print something for state in df[‘STNAME’].unique(): # We’ll just calculate the average using numpy for this particular state avg = np.average(df.where(df[‘STNAME’]==state).dropna()[‘CENSUS2010POP’]) # And we’ll print it to the screen print(‘Counties in state’ + state + ’ have an average population of ’ + str(avg)) 1.9 Lambda Function df.apply(lambda x: np.max(x[rows]), axis=1) df[‘quantity’] = df[‘quantity’].apply(lambda x: x*-1) my_function = lambda a, b, c : a + b my_function(1, 2, 3) (get 3) 1.10 General Funtions def split_title_and_name(person): title = person.split()[0] lastname = person.split()[-1] return ‘{} {}’.format(title, lastname) def add_lists(outside_lst): result_lst = list() for j in range(0, len(outside_lst[0])): tmp = 0 for i in range(0, len(outside_lst)): tmp = tmp + outside_lst[i][j] result_lst.append(tmp) return result_lst "],["natual-language-processingnlp.html", "Chapter 2 Natual Language Processing(NLP) 2.1 Regex/Regular Expression 2.2 Online Scraping 2.3 Descriptive Statistics of Text Data 2.4 Text Data Cleaning 2.5 Textual Cues Tagging 2.6 Dependency Relations in Spacy 2.7 Tag symbols", " Chapter 2 Natual Language Processing(NLP) 2.1 Regex/Regular Expression 2.1.1 Symbols [] A set of characters Signals a special sequence (can also be used to escape special characters) . Any character (except newline character) ^ Starts with $ Ends with * Zero or more occurrences + One or more occurrences ? an occurrence of atom, or the null string. {} Exactly the specified number of occurrences ?= look ahead | Either or () Capture and group Example of (): Only pull out the part in () re.findall(r’ (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) ’, dateStr) [‘Oct’] A special sequence is a followed by one of the characters in the list below, and has a special meaning: Character Description Returns a match if the specified characters are at the beginning of the string eturns a match where the specified characters are at the beginning or at the end of a word (the “r” in the beginning is making sure that the string is being treated as a “raw string”) Returns a match where the specified characters are present, but NOT at the beginning (or at the end) of a word (the “r” in the beginning is making sure that the string is being treated as a “raw string”) eturns a match where the string contains digits (numbers from 0-9) Returns a match where the string DOES NOT contain digits Returns a match where the string contains a white space character Returns a match where the string DOES NOT contain a white space character Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character) Returns a match where the string DOES NOT contain any word characters Returns a match if the specified characters are at the end of the string Sets A set is a set of characters inside a pair of square brackets [] with a special meaning. [arn] Returns a match where one of the specified characters (a, r, or n) are present e.g. [ao] means any character from ‘a’ and ‘o’ (or both) hence only ‘bat’ and ‘bot’ would be extracted import re string = ‘bat, lat, mat, bet, let, met, bit, lit, mit, bot, lot, mot’ result = re.findall(‘b[ao]t’, string) print(result) [‘bat’, ‘bot’] [/-] / or - [a-n] Returns a match for any lower case character, alphabetically between a and n [^arn] Returns a match for any character EXCEPT a, r, and n When ^ is used outside square brackets, it denotes that the expression inside the brackets should not be extracted from the string [0123] Returns a match where any of the specified digits (0, 1, 2, or 3) are present [0-9] Returns a match for any digit between 0 and 9 [0-5][0-9] Returns a match for any two-digit numbers from 00 and 59 [a-zA-Z] Returns a match for any character alphabetically between a and z, lower case OR upper case [+] In sets, +, *, ., |, (), $,{} has no special meaning, so [+] means: return a match for any + character in the string re.findall(‘[cd]?[cde]?’, ‘ccc dd ee’) [‘cc’, ‘c’, ’‘, ’dd’, ’‘, ’e’, ‘e’, ’’] This regex pattern looks complicated: ‘[cd]?[cde]?’. But is it really? Let’s break it down step-by-step: • The first part of the regex [cd]? defines a character class [cd] which reads as “match either c or d”. The question mark quantifier indicates that you want to match either one or zero occurrences of this pattern. • The second part of the regex [cde]? defines a character class [cde] which reads as “match either c, d, or e”. Again, the question mark indicates the zero-or-one matching requirement. re.search returns match or not. re.find can extract the matched patterns. .str.contains() can only be used on a Pandas series, not after extracting an individual string. re.split #split by something in the () isalpha() method returns True if all the characters are alphabet letters (a-z) 2.1.2 Symple aplications import re txt = “The rain in Spain” x = re.search(“^The.*Spain$”, txt) Print a list of all matches: txt = “The rain in Spain” x = re.findall(“ai”, txt) print(x) Return an empty list if no match was found: txt = “The rain in Spain” x = re.findall(“Portugal”, txt) print(x) Search for the first white-space character in the string: txt = “The rain in Spain” x = re.search(“”, txt) print(“The first white-space character is located in position:”, x.start()) Make a search that returns no match: txt = “The rain in Spain” x = re.search(“Portugal”, txt) print(x) Split at each white-space character: txt = “The rain in Spain” x = re.split(“”, txt) print(x) Split the string only at the first occurrence: txt = “The rain in Spain” x = re.split(“”, txt, 1) print(x) Replace every white-space character with the number 9: import re txt = “The rain in Spain” x = re.sub(“”, “9”, txt) print(x) Replace the first 2 occurrences: txt = “The rain in Spain” x = re.sub(“”, “9”, txt, 2) print(x) Do a search that will return a Match Object: txt = “The rain in Spain” x = re.search(“ai”, txt) print(x) #this will print an object The Match object has properties and methods used to retrieve information about the search, and the result: .span() returns a tuple containing the start-, and end positions of the match. .string returns the string passed into the function .group() returns the part of the string where there was a match Examples: Print the position (start- and end-position) of the first match occurrence. The regular expression looks for any words that starts with an upper case “S”: import re txt = “The rain in Spain” x = re.search(r”+“, txt) print(x.span()) Print the string passed into the function: txt = “The rain in Spain” x = re.search(r”+“, txt) print(x.string) Print the part of the string where there was a match. The regular expression looks for any words that starts with an upper case “S”: import re txt = “The rain in Spain” x = re.search(r”+“, txt) print(x.group()) in function ss = ‘I like it. Let us proceed the order now. Will you help me with that?’ if cues in ss: print(‘YesYes!’) else: print (‘Ohnono’) 2.2 Online Scraping import time from selenium import webdriver import re from selenium.common.exceptions import NoSuchElementException import json from random import randint import csv data = [] options = webdriver.ChromeOptions() options.add_argument(‘headless’) browser = webdriver.Chrome(r’C:\\location\\chromedriver.exe’, options=options) data = [] options = webdriver.ChromeOptions() options.add_argument(‘headless’) browser = webdriver.Chrome(r’C:\\location\\chromedriver.exe’, options=options) 2.3 Descriptive Statistics of Text Data 2.3.1 Word Frequencies import nltk words = nltk.tokenize.word_tokenize(str) fd = nltk.FreqDist(words) def Convert(tup, di): di = dict(tup) return di dictionary = {} wordfreq = (Convert(fd.items(), dictionary)) newwordfreq = dict(sorted(wordfreq.items(), key=lambda item: item[1], reverse=True)) file = open(“DictFile.txt”,“w”) for key, value in newwordfreq.items(): file.write(&#39; %-*s %s\\n&#39; % (20,key, value)) #%-*s and 20 are for setting the space between words and numbers file.close() 2.4 Text Data Cleaning Insert Regex to textual cues def clean_keywords_bypattern(df,keywords,pattern_keyword, pattern_type): df[keywords] = df[keywords].str.strip().str.lower() df[keywords] = df[keywords].str.replace(’*’,“(+)?(++){0,5}?”) keywords = list(df.loc[(df[pattern_keyword] == pattern_type),keywords]) keywords = [(r’[|]?‘+ x + r’[|]?‘) for x in keywords if str(x) != ’nan’] return keywords Clean contractions in the data def clean_contractions(data_source, variable_to_clean): CONTRACTION_MAP = { “ain’t”: “am not / are not / is not / has not / have not”, “aren’t”: “are not / am not”, “can’t”: “cannot”, “can’t’ve”: “cannot have”, “’cause”: “because”, “could’ve”: “could have”, “couldn’t”: “could not”, “couldn’t’ve”: “could not have”, “didn’t”: “did not”, “doesn’t”: “does not”, “don’t”: “do not”, “hadn’t”: “had not”, “hadn’t’ve”: “had not have”, “hasn’t”: “has not”, “haven’t”: “have not”, “he’d”: “he had / he would”, “he’d’ve”: “he would have”, “he’ll”: “he he will”, “he’ll’ve”: “he shall have / he will have”, “he’s”: “he has / he is”, “how’d”: “how did”, “how’d’y”: “how do you”, “how’ll”: “how will”, “how’s”: “how has / how is / how does”, “I’d”: “I had / I would”, “I’d’ve”: “I would have”, “I’ll”: “I shall / I will”, “I’ll’ve”: “I shall have / I will have”, “I’m”: “I am”, “I’ve”: “I have”, “isn’t”: “is not”, “it’d”: “it had / it would”, “it’d’ve”: “it would have”, “it’ll”: “it shall / it will”, “it’ll’ve”: “it shall have / it will have”, “it’s”: “it has / it is”, “let’s”: “let us”, “ma’am”: “madam”, “mayn’t”: “may not”, “might’ve”: “might have”, “mightn’t”: “might not”, “mightn’t’ve”: “might not have”, “must’ve”: “must have”, “mustn’t”: “must not”, “mustn’t’ve”: “must not have”, “needn’t”: “need not”, “needn’t’ve”: “need not have”, “o’clock”: “of the clock”, “oughtn’t”: “ought not”, “oughtn’t’ve”: “ought not have”, “shan’t”: “shall not”, “sha’n’t”: “shall not”, “shan’t’ve”: “shall not have”, “she’d”: “she had / she would”, “she’d’ve”: “she would have”, “she’ll”: “she shall / she will”, “she’ll’ve”: “she shall have / she will have”, “she’s”: “she has / she is”, “should’ve”: “should have”, “shouldn’t”: “should not”, “shouldn’t’ve”: “should not have”, “so’ve”: “so have”, “so’s”: “so as / so is”, “that’d”: “that would / that had”, “that’d’ve”: “that would have”, “that’s”: “that has / that is”, “there’d”: “there had / there would”, “there’d’ve”: “there would have”, “there’s”: “there has / there is”, “they’d”: “they had / they would”, “they’d’ve”: “they would have”, “they’ll”: “they shall / they will”, “they’ll’ve”: “they shall have / they will have”, “they’re”: “they are”, “they’ve”: “they have”, “to’ve”: “to have”, “wasn’t”: “was not”, “we’d”: “we had / we would”, “we’d’ve”: “we would have”, “we’ll”: “we will”, “we’ll’ve”: “we will have”, “we’re”: “we are”, “we’ve”: “we have”, “weren’t”: “were not”, “what’ll”: “what shall / what will”, “what’ll’ve”: “what shall have / what will have”, “what’re”: “what are”, “what’s”: “what has / what is”, “what’ve”: “what have”, “when’s”: “when has / when is”, “when’ve”: “when have”, “where’d”: “where did”, “where’s”: “where has | where is”, “where’ve”: “where have”, “who’ll”: “who shall / who will”, “who’ll’ve”: “who shall have / who will have”, “who’s”: “who has / who is”, “who’ve”: “who have”, “why’s”: “why has / why is”, “why’ve”: “why have”, “will’ve”: “will have”, “won’t”: “will not”, “won’t’ve”: “will not have”, “would’ve”: “would have”, “wouldn’t”: “would not”, “wouldn’t’ve”: “would not have”, “y’all”: “you all”, “y’all’d”: “you all would”, “y’all’d’ve”: “you all would have”, “y’all’re”: “you all are”, “y’all’ve”: “you all have”, “you’d”: “you had | you would”, “you’d’ve”: “you would have”, “you’ll”: “you shall | you will”, “you’ll’ve”: “you shall have | you will have”, “you’re”: “you are”, “you’ve”: “you have”} # contraction definition. data_source[variable_to_clean] = data_source[variable_to_clean].apply(lambda x:str(x).replace(&quot;’&quot;,&quot;&#39;&quot;)) # replace the wrong quotation mark by the correct one for contraction in CONTRACTION_MAP: data_source[variable_to_clean] = data_source[variable_to_clean].apply(lambda x:str(x).replace(contraction,CONTRACTION_MAP[contraction])) return data_source Add space for those sentences without space in between. dfsource[‘text’] = dfsource[‘text’].apply(lambda x: re.sub(r’([a-z]|]).([A-Z]|[)‘, r’\\1. \\2’, str(x))) Filter text column, extract cues text = df[‘variable_name’] cues = text.str.extract(r”(1)(.+)“, expand=True).dropna()[1] Tokenlize dfsource[‘text’] = dfsource.apply(lambda row: sent_tokenize(row[‘text’]), axis=1) Autocorrection 1.By words with a text file that defines word frequencies: one-grams.txt import functools, math class OneGramDist(dict): def init(self, filename): self.gramCount = 0 for line in open(filename): (word, count) = line[:-1].split(&#39;\\t&#39;) self[word] = int(count) self.gramCount += self[word] def call(self, key): if key in self: return float(self[key]) / self.gramCount else: return 1.0 / (self.gramCount * 10**(len(key)-2)) singleWordProb = OneGramDist(‘one-grams.txt’) def wordSeqFitness(words): return sum(math.log10(singleWordProb(w)) for w in words) def memoize(f): cache = {} def memoizedFunction(args): if args not in cache: cache[args] = f(args) return cache[args] memoizedFunction.cache = cache return memoizedFunction @memoize def segment(word): if not word: return [] word = word.lower() # change to lower case allSegmentations = [[first] + segment(rest) for (first,rest) in splitPairs(word)] return max(allSegmentations, key = wordSeqFitness) def splitPairs(word, maxLen=20): return [(word[:i+1], word[i+1:]) for i in range(max(len(word), maxLen))] @memoize def segmentWithProb(word): segmented = segment(word) return (wordSeqFitness(segmented), segmented) def correct_string(string): x = string.split() correct_string=” ” for i in x: a=segment(i) b=’ ‘.join(a) correct_string=correct_string+’ ’+b return correct_string correct_string(” He is upset because a supervisor was suppose to call him back as well about the password change.”) Existing packages from textblob import TextBlob txt=“He is upset because a supervisor was suppose to call him back as well about the password change.” b = TextBlob(txt) print(“after spell correction:”+str(b.correct())) from autocorrect import Speller spell = Speller(lang=‘en’) spell(“He is upset because a supervisor was suppose to call him back as well about the password change.”) import contextualSpellCheck nlp = spacy.load(‘en_core_web_sm’) contextualSpellCheck.add_to_pipe(nlp) doc = nlp(‘This is my neww job. My firsrt neme is Jack.’) print(doc._.outcome_spellCheck) 2.By context import language_tool_python tool = language_tool_python.LanguageTool(‘en-US’) text = “He is upset because a supervisor was suppose to call him back as well about the password change.” tool.correct(text) from gramformer import Gramformer import torch def set_seed(seed): torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) set_seed(1212) gf = Gramformer(models = 1, use_gpu=False) # 1=corrector, 2=detector influent_sentences = [ “He are moving here.”, “He is upset because a supervisor was suppose to call him back as well about the password change.”] for influent_sentence in influent_sentences: corrected_sentences = gf.correct(influent_sentence, max_candidates=1) print(“[Input]”, influent_sentence) for corrected_sentence in corrected_sentences: print(“[Correction]”,corrected_sentence) print(“-” *100) Lemmatization my_str = dfsource.iloc[16,1] doc = nlp(my_str) sentences_lemmata_list = [sentence.lemma_ for sentence in nlp(doc).sents] print(sentences_lemmata_list) Find the subjects subj =[tok for tok in doc if ((tok.dep_ == “nsubjpass”) | (tok.dep_ == “nsubj”))] 2.5 Textual Cues Tagging Build a textual cue dictionary vlist = DICC.columns.tolist() d = {} i = 0 for v in vlist: i = i + 1 d[“V” + str(i)] = clean_cues(DICC,v) newkeys = vlist vals = list(d.values()) cuedic = {k: v for k, v in zip(newkeys, vals)} Inclusion&amp;Exclusion def create_df_name(df_source, role_name, keyword1,keyword2,keyword3,keyword_exclusion,description): # depending on the number of word pairs and exclusion rule, they will form the differnt pairs of words to tag the focal tactic. # df_source is the list of sentences in the excel to be tagged. if keyword2 is None and keyword3 is None: df_name = (df_source.loc[(df_source.Role == role_name) &amp; (df_source.Sentences.str.contains(‘|’.join(keyword1),na=False)) &amp; (~df_source.Sentences.str.contains(‘|’.join(keyword_exclusion),na=False)) ,[‘InteractionID’,‘Role’,‘Sequence’,‘Sentences’]]) elif keyword3 is None: df_name = (df_source.loc[(df_source.Role == role_name) &amp; (df_source.Sentences.str.contains(‘|’.join(keyword1),na=False)) &amp; (df_source.Sentences.str.contains(‘|’.join(keyword2),na=False)) &amp; (~df_source.Sentences.str.contains(‘|’.join(keyword_exclusion),na=False)) ,[‘InteractionID’,‘Role’,‘Sequence’,‘Sentences’]]) else: df_name = (df_source.loc[(df_source.Role == role_name) &amp; (df_source.Sentences.str.contains(‘|’.join(keyword1),na=False)) &amp; (df_source.Sentences.str.contains(‘|’.join(keyword2),na=False)) &amp; (df_source.Sentences.str.contains(‘|’.join(keyword3),na=False)) &amp; (~df_source.Sentences.str.contains(‘|’.join(keyword_exclusion),na=False)) ,[‘InteractionID’,‘Role’,‘Sequence’,‘Sentences’]]) df_name.drop_duplicates(subset = (“InteractionID”,“Sequence”),keep=‘first’,inplace=True) df_name[‘Type’] = description return df_name Tag two variables def tag_with_cues(data_source, tex_to_tag, variable_name_cues1, variable_name_cues2, checked_cues1, checked_cues2): data_source[checked_cues1] = data_source[tex_to_tag].str.contains(‘|’.join(variable_name_cues1),na=False) data_source.replace({False: 0, True: 1}, inplace=True)#replace true or false to 1 and 0 data_source[checked_cues1] = data_source.groupby([‘ID1’, ‘ID2’, ‘ID3’])[checked_cues1].transform(‘max’)#fill the groups with 1 as long as there is a least a 1 data_source[checked_cues2] = data_source[tex_to_tag].str.contains(&#39;|&#39;.join(variable_name_cues2),na=False) data_source.replace({False: 0, True: 1}, inplace=True)#replace true or false to 1 and 0 data_source[checked_cues2] = data_source.groupby([&#39;ID1&#39;, &#39;ID2&#39;, &#39;ID3&#39;])[checked_cues2].transform(&#39;max&#39;) return data_source Verify the number of obs and groups def length_groups(data_to_check): obslength = len(data_to_check) groupnumbers = data_to_check.groupby([‘ID1’, ‘ID2’, ‘ID3’]).ngroups print(“obslength:”,obslength,“;”,“groupnumbers:”,groupnumbers) Tag ignoring word order re.findall(r’^(?=.permit)(?=.not)(?=.*his).+$’, my_str) Random sampling def random_sample(data_to_sample, samplesize): #generate the distinct combo IDs distinctcomboIDs = data_to_sample.groupby([‘ID1’, ‘ID2’, ‘ID3’]).size().reset_index().rename(columns={0:‘count’}) random.seed(82) #set a seed nsample = distinctcomboIDs.sample(n = samplesize) extracted_sample = data_to_sample[(data_to_sample[‘ID1’]).isin(nsample[‘ID1’]) &amp; (data_tagged_positve[‘ID2’]).isin(nsample[‘ID2’]) &amp; (data_tagged_positve[‘ID3’]).isin(nsample[‘ID3’])] return extracted_sample Simple sampling random_sample(df, 100) 2.5.1 Syntax len(text7) #include all the len(set(text7)) #unique, not include duplicates dfsource = dfsource.applymap(str) 2.6 Dependency Relations in Spacy import spacy nlp = spacy.load(“en_core_web_sm”) sm means small, lg means large, and m… Check NLP Pipe Names print(nlp.pipe_names) Add NLP Pipe nlp = spacy.load(‘en_core_web_sm’) contextualSpellCheck.add_to_pipe(nlp) 2.7 Tag symbols CC: Coordinating conjunction CD: Cardinal number DT: Determiner EX: Existential there FW: Foreign word IN: Preposition or subordinating conjunction JJ: Adjective VP: Verb Phrase JJR: Adjective, comparative JJS: Adjective, superlative LS: List item marker MD: Modal NN: Noun, singular or mass NNS: Noun, plural PP: Preposition Phrase NNP: Proper noun, singular Phrase NNPS: Proper noun, plural PDT: Pre determiner POS: Possessive ending PRP: Personal pronoun Phrase PRP: Possessive pronoun Phrase RB: Adverb RBR: Adverb, comparative RBS: Adverb, superlative RP: Particle S: Simple declarative clause SBAR: Clause introduced by a (possibly empty) subordinating conjunction SBARQ: Direct question introduced by a wh-word or a wh-phrase. SINV: Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal. SQ: Inverted yes/no question, or main clause of a wh-question, following the wh-phrase in SBARQ. SYM: Symbol VBD: Verb, past tense VBG: Verb, gerund or present participle VBN: Verb, past participle VBP: Verb, non-3rd person singular present VBZ: Verb, 3rd person singular present WDT: Wh-determiner WP: Wh-pronoun WP: Possessive wh-pronoun WRB: Wh-adverb Define Doc doc = nlp(“Apple is looking at buying U.K. startup for $1 billion”) #any sentence Check Tokens/Nodes for word in doc: print(word.orth_) Check All the Tags for token in doc: print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop) Print the tags in a Pretty Way for sent in doc.sents: for token in sent: print(“{}”.format(token.i, token.text, token.head, token.dep_)) Build the Dependency Relationship to a Tree from nltk import Tree def to_nltk_tree(node): if node.n_lefts + node.n_rights &gt; 0: return Tree(node.orth_, [to_nltk_tree(child) for child in node.children]) else: return node.orth_ [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents] Check the Root Word for sent in doc.sents: print(sent.root) Check the Index of Each Word Token for word in doc: print(word.orth) OR for token in doc: print((‘{0}’.format(token.lower_))) Check the Number of Left Nodes for word in doc: print(word.n_lefts) Check the Right Nodes for word in doc: print(word.n_rights) "],["networks.html", "Chapter 3 Networks 3.1 Concepts 3.2 Generate a Network Graph 3.3 Send Generated Text Dependency Tree to NetworkX 3.4 Algorithms", " Chapter 3 Networks 3.1 Concepts Reference: https://networkx.org/ 3.2 Generate a Network Graph import networkx as nx G = nx.Graph() G.add_nodes_from([2, 3]) G.add_nodes_from([ (4, {“color”: “red”}), (5, {“color”: “green”})]) import matplotlib.pyplot as plt G = nx.dodecahedral_graph() shells = [[2, 3, 4, 5, 6], [8, 1, 0, 19, 18, 17, 16, 15, 14, 7], [9, 10, 11, 12, 13]] nx.draw_shell(G, nlist=shells, **options) 3.3 Send Generated Text Dependency Tree to NetworkX from nltk import Tree def to_nltk_tree(node): if node.n_lefts + node.n_rights &gt; 0: return Tree(node.orth_, [to_nltk_tree(child) for child in node.children]) else: return node.orth_ [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents] import networkx as nx edges = [] for token in doc: for child in token.children: edges.append((‘{0}’.format(token.lower_), ‘{0}’.format(child.lower_))) graph = nx.Graph(edges) 3.4 Algorithms Check the Distance between Pairs of Nodes entity1 = ‘looking’.lower() entity2 = ‘billion’ print(nx.shortest_path_length(graph, source=entity1, target=entity2)) Check the Path print(nx.shortest_path(graph, source=entity1, target=entity2)) Use Dijkstra Algorithm nx.dijkstra_path_length(graph, ‘looking’, ‘billion’, weight=‘weight’) "],["parallel-processing.html", "Chapter 4 Parallel Processing", " Chapter 4 Parallel Processing ↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
